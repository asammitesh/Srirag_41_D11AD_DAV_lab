{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOf11/eDNKm1YoXlb6MITK1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asammitesh/Srirag_41_D11AD_DAV_lab/blob/main/DAV_EX7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment - 7\n",
        "Aim: Perform the steps involved in Text Analytics in Python & R\n",
        "\n",
        "Task to be performed :\n",
        "\n",
        "Explore Top-5 Text Analytics Libraries in Python (w.r.t Features & Applications)\n",
        "\n",
        "Explore Top-5 Text Analytics Libraries in R (w.r.t Features & Applications)\n",
        "\n",
        "Perform the following experiments using Python & R\n",
        "Tokenization (Sentence & Word)\n",
        "\n",
        "Frequency Distribution\n",
        "\n",
        "Remove stopwords & punctuations\n",
        "\n",
        "Lexicon Normalization (Stemming, Lemmatization)\n",
        "\n",
        "Part of Speech tagging\n",
        "\n",
        "Named Entity Recognization\n",
        "\n",
        "# **Scrape data from a website**\n",
        "\n"
      ],
      "metadata": {
        "id": "Lx9BmhT-KfcO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o48EFgVHKbij",
        "outputId": "25dfc5d3-8fcc-4fa5-e70c-7070bb124130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A fictional bookstore that desperately wants to be scraped. It's a safe place for beginners learning web scraping and for developers validating their scraping technologies as well. Available at: books.toscrape.com\n",
            "A website that lists quotes from famous people. It has many endpoints showing the quotes in many different ways, each of them including new scraping challenges for you, as described below.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Specify the URL of the webpage to scrape\n",
        "url = 'https://toscrape.com/'\n",
        "x=''\n",
        "# Send an HTTP GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the webpage\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find and extract specific elements or data from the webpage\n",
        "    # Example: Extract all <p> tags\n",
        "    paragraphs = soup.find_all('p')\n",
        "\n",
        "    # Print the extracted data\n",
        "    for paragraph in paragraphs:\n",
        "        print(paragraph.text)\n",
        "        x += paragraph.text\n",
        "else:\n",
        "    print('Failed to retrieve the webpage. Status code:', response.status_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TOKENIZER**"
      ],
      "metadata": {
        "id": "Z7UrufdXLGqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "text = x\n",
        "sentences = sent_tokenize(text)\n",
        "words = word_tokenize(text)\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7n-j4eXLLBf",
        "outputId": "9a89c03c-99d4-4b7e-afcd-a99a713c6f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['A fictional bookstore that desperately wants to be scraped.', \"It's a safe place for beginners learning web scraping and for developers validating their scraping technologies as well.\", 'Available at: books.toscrape.comA website that lists quotes from famous people.', 'It has many endpoints showing the quotes in many different ways, each of them including new scraping challenges for you, as described below.']\n",
            "Words: ['A', 'fictional', 'bookstore', 'that', 'desperately', 'wants', 'to', 'be', 'scraped', '.', 'It', \"'s\", 'a', 'safe', 'place', 'for', 'beginners', 'learning', 'web', 'scraping', 'and', 'for', 'developers', 'validating', 'their', 'scraping', 'technologies', 'as', 'well', '.', 'Available', 'at', ':', 'books.toscrape.comA', 'website', 'that', 'lists', 'quotes', 'from', 'famous', 'people', '.', 'It', 'has', 'many', 'endpoints', 'showing', 'the', 'quotes', 'in', 'many', 'different', 'ways', ',', 'each', 'of', 'them', 'including', 'new', 'scraping', 'challenges', 'for', 'you', ',', 'as', 'described', 'below', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency **Distribution**"
      ],
      "metadata": {
        "id": "Onxs7vf6LSB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Assuming 'words' is a list of words\n",
        "fdist = FreqDist(words)\n",
        "\n",
        "# Print frequency counts for each word\n",
        "for word, frequency in fdist.items():\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nfa3GGv5LUPg",
        "outputId": "8e0c5da3-9196-40ce-827c-1bdab7694cd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A: 1\n",
            "fictional: 1\n",
            "bookstore: 1\n",
            "that: 2\n",
            "desperately: 1\n",
            "wants: 1\n",
            "to: 1\n",
            "be: 1\n",
            "scraped: 1\n",
            ".: 4\n",
            "It: 2\n",
            "'s: 1\n",
            "a: 1\n",
            "safe: 1\n",
            "place: 1\n",
            "for: 3\n",
            "beginners: 1\n",
            "learning: 1\n",
            "web: 1\n",
            "scraping: 3\n",
            "and: 1\n",
            "developers: 1\n",
            "validating: 1\n",
            "their: 1\n",
            "technologies: 1\n",
            "as: 2\n",
            "well: 1\n",
            "Available: 1\n",
            "at: 1\n",
            ":: 1\n",
            "books.toscrape.comA: 1\n",
            "website: 1\n",
            "lists: 1\n",
            "quotes: 2\n",
            "from: 1\n",
            "famous: 1\n",
            "people: 1\n",
            "has: 1\n",
            "many: 2\n",
            "endpoints: 1\n",
            "showing: 1\n",
            "the: 1\n",
            "in: 1\n",
            "different: 1\n",
            "ways: 1\n",
            ",: 2\n",
            "each: 1\n",
            "of: 1\n",
            "them: 1\n",
            "including: 1\n",
            "new: 1\n",
            "challenges: 1\n",
            "you: 1\n",
            "described: 1\n",
            "below: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove stopwords & **punctuations**"
      ],
      "metadata": {
        "id": "_MGgxjXBLbLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words and word not in string.punctuation]\n",
        "print(\"Filtered Words:\", filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E78OtXLDLll4",
        "outputId": "ca83ceb6-5194-4d4f-8d9d-7b4bfeb276a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Words: ['fictional', 'bookstore', 'desperately', 'wants', 'scraped', \"'s\", 'safe', 'place', 'beginners', 'learning', 'web', 'scraping', 'developers', 'validating', 'scraping', 'technologies', 'well', 'Available', 'books.toscrape.comA', 'website', 'lists', 'quotes', 'famous', 'people', 'many', 'endpoints', 'showing', 'quotes', 'many', 'different', 'ways', 'including', 'new', 'scraping', 'challenges', 'described']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lexicon Normalization (Stemming, **Lemmatization**"
      ],
      "metadata": {
        "id": "f4Lo3y3aLpvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download WordNet corpus\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize the text and remove punctuation\n",
        "words = [word for word in word_tokenize(text) if word not in string.punctuation]\n",
        "\n",
        "# Initialize stemmer and lemmatizer\n",
        "porter = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Perform stemming and lemmatization\n",
        "stemmed_words = [porter.stem(word) for word in words]\n",
        "lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
        "\n",
        "# Print the results\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfnlcfHgLule",
        "outputId": "339e1dab-d8d8-4ce5-a719-f2e880867452"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['a', 'fiction', 'bookstor', 'that', 'desper', 'want', 'to', 'be', 'scrape', 'it', \"'s\", 'a', 'safe', 'place', 'for', 'beginn', 'learn', 'web', 'scrape', 'and', 'for', 'develop', 'valid', 'their', 'scrape', 'technolog', 'as', 'well', 'avail', 'at', 'books.toscrape.coma', 'websit', 'that', 'list', 'quot', 'from', 'famou', 'peopl', 'it', 'ha', 'mani', 'endpoint', 'show', 'the', 'quot', 'in', 'mani', 'differ', 'way', 'each', 'of', 'them', 'includ', 'new', 'scrape', 'challeng', 'for', 'you', 'as', 'describ', 'below']\n",
            "Lemmatized Words: ['a', 'fictional', 'bookstore', 'that', 'desperately', 'want', 'to', 'be', 'scraped', 'it', \"'s\", 'a', 'safe', 'place', 'for', 'beginner', 'learning', 'web', 'scraping', 'and', 'for', 'developer', 'validating', 'their', 'scraping', 'technology', 'a', 'well', 'available', 'at', 'books.toscrape.coma', 'website', 'that', 'list', 'quote', 'from', 'famous', 'people', 'it', 'ha', 'many', 'endpoint', 'showing', 'the', 'quote', 'in', 'many', 'different', 'way', 'each', 'of', 'them', 'including', 'new', 'scraping', 'challenge', 'for', 'you', 'a', 'described', 'below']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part of Speech **tagging**"
      ],
      "metadata": {
        "id": "P0B-VynKMI5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_tags = nltk.pos_tag(filtered_words)\n",
        "print(\"Part of Speech Tags:\", pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3c8sHT-L3jf",
        "outputId": "cca523fa-71f6-4f67-aa4a-b84af67a6b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part of Speech Tags: [('fictional', 'JJ'), ('bookstore', 'NN'), ('desperately', 'RB'), ('wants', 'VBZ'), ('scraped', 'VBD'), (\"'s\", 'POS'), ('safe', 'JJ'), ('place', 'NN'), ('beginners', 'NNS'), ('learning', 'VBG'), ('web', 'JJ'), ('scraping', 'VBG'), ('developers', 'NNS'), ('validating', 'VBG'), ('scraping', 'NN'), ('technologies', 'NNS'), ('well', 'RB'), ('Available', 'NNP'), ('books.toscrape.comA', 'NN'), ('website', 'NN'), ('lists', 'NNS'), ('quotes', 'VBZ'), ('famous', 'JJ'), ('people', 'NNS'), ('many', 'JJ'), ('endpoints', 'NNS'), ('showing', 'VBG'), ('quotes', 'NNS'), ('many', 'JJ'), ('different', 'JJ'), ('ways', 'NNS'), ('including', 'VBG'), ('new', 'JJ'), ('scraping', 'NN'), ('challenges', 'NNS'), ('described', 'VBD')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity **Recognization**"
      ],
      "metadata": {
        "id": "MoJqUb5xMas2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text2=\"New York City, often simply referred to as New York, is the most populous city in the United States. It is located in the northeastern region of the country and is known for its iconic landmarks such as the Statue of Liberty, Times Square, and Central Park. The city is a major hub for finance, culture, and entertainment, attracting millions of tourists every year. Some of the world's leading companies and institutions are headquartered in New York City, making it a global center for commerce and innovation.\"\n",
        "doc = nlp(text2)\n",
        "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
        "print(\"Named Entities:\", entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVC-h-ODMimQ",
        "outputId": "6f979bb3-8225-4465-f225-4cda0a62872b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Named Entities: [('New York City', 'GPE'), ('New York', 'GPE'), ('the United States', 'GPE'), ('the Statue of Liberty', 'FAC'), ('Times Square', 'FAC'), ('Central Park', 'LOC'), ('millions', 'CARDINAL'), ('New York City', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IN R\n",
        "# **TOKENIZER**"
      ],
      "metadata": {
        "id": "FFp0cQARMyf8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the tokenizers package if not already installed\n",
        "if (!require(tokenizers)) {\n",
        "  install.packages(\"tokenizers\")\n",
        "}\n",
        "\n",
        "# Load the tokenizers package\n",
        "library(tokenizers)\n",
        "\n",
        "text <- readline(prompt = \"Enter text: \")\n",
        "\n",
        "word_tokens <- unlist(tokenize_words(text))\n",
        "sentence_tokens <- unlist(tokenize_sentences(text))\n",
        "\n",
        "cat(\"\\nTokenized words:\\n\")\n",
        "print(word_tokens)\n",
        "\n",
        "cat(\"\\nTokenized sentences:\\n\")\n",
        "print(sentence_tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnQiM6IBM2-Y",
        "outputId": "4d81d880-554c-46ab-c904-8fb1453299de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: tokenizers\n",
            "\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘tokenizers’”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘Rcpp’, ‘SnowballC’\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: aur bhi dukh hai zamane mai mohabbat ke siva , raheten aur bhi hai vasl ke rahat ke siva\n",
            "\n",
            "Tokenized words:\n",
            " [1] \"aur\"      \"bhi\"      \"dukh\"     \"hai\"      \"zamane\"   \"mai\"     \n",
            " [7] \"mohabbat\" \"ke\"       \"siva\"     \"raheten\"  \"aur\"      \"bhi\"     \n",
            "[13] \"hai\"      \"vasl\"     \"ke\"       \"rahat\"    \"ke\"       \"siva\"    \n",
            "\n",
            "Tokenized sentences:\n",
            "[1] \"aur bhi dukh hai zamane mai mohabbat ke siva , raheten aur bhi hai vasl ke rahat ke siva\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Frequency **Distribution**"
      ],
      "metadata": {
        "id": "vXHgjGkgONuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq <- table(word_tokens)\n",
        "\n",
        "print(\"Most common words:\")\n",
        "print(head(sort(word_freq, decreasing = TRUE), 2))\n",
        "\n",
        "print(\"Frequency of each word:\")\n",
        "print(word_freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uf1LpVc5OTo5",
        "outputId": "4c6799b2-330e-47a7-e2df-c10299c57782"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Most common words:\"\n",
            "word_tokens\n",
            " ke aur \n",
            "  3   2 \n",
            "[1] \"Frequency of each word:\"\n",
            "word_tokens\n",
            "     aur      bhi     dukh      hai       ke      mai mohabbat    rahat \n",
            "       2        2        1        2        3        1        1        1 \n",
            " raheten     siva     vasl   zamane \n",
            "       1        2        1        1 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove stopwords & **punctuations**"
      ],
      "metadata": {
        "id": "BZQw9V3-O7T0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the tm package if not already installed\n",
        "if (!require(tm)) {\n",
        "  install.packages(\"tm\")\n",
        "}\n",
        "\n",
        "# Load the tm package\n",
        "library(tm)\n",
        "\n",
        "filtered_tokens <- word_tokens[!word_tokens %in% stopwords(\"en\")]\n",
        "\n",
        "print(\"Filtered Tokens:\")\n",
        "print(filtered_tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RkxmljaPCum",
        "outputId": "7593f968-b66e-4bf0-c2cb-08a8171b6a45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: tm\n",
            "\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘tm’”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘NLP’, ‘slam’, ‘BH’\n",
            "\n",
            "\n",
            "Loading required package: NLP\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Filtered Tokens:\"\n",
            " [1] \"aur\"      \"bhi\"      \"dukh\"     \"hai\"      \"zamane\"   \"mai\"     \n",
            " [7] \"mohabbat\" \"ke\"       \"siva\"     \"raheten\"  \"aur\"      \"bhi\"     \n",
            "[13] \"hai\"      \"vasl\"     \"ke\"       \"rahat\"    \"ke\"       \"siva\"    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lexicon Normalization (Stemming, Lemmatization)**bold text**"
      ],
      "metadata": {
        "id": "FyZHiGF2PrIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemming <- function(text) {\n",
        "  corpus <- Corpus(VectorSource(text))\n",
        "  corpus <- tm_map(corpus, stemDocument)\n",
        "  return(corpus)\n",
        "}\n",
        "\n",
        "stemmed_corpus <- stemming(filtered_tokens)\n",
        "\n",
        "print(\"Stemmed Tokens:\")\n",
        "print(unlist(sapply(stemmed_corpus, as.character)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FmxbFmhPqXo",
        "outputId": "0f0de24b-fa0f-4742-ab17-afc538e50a29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in tm_map.SimpleCorpus(corpus, stemDocument):\n",
            "“transformation drops documents”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Stemmed Tokens:\"\n",
            " [1] \"aur\"      \"bhi\"      \"dukh\"     \"hai\"      \"zaman\"    \"mai\"     \n",
            " [7] \"mohabbat\" \"ke\"       \"siva\"     \"raheten\"  \"aur\"      \"bhi\"     \n",
            "[13] \"hai\"      \"vasl\"     \"ke\"       \"rahat\"    \"ke\"       \"siva\"    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the textstem package if not already installed\n",
        "if (!require(textstem)) {\n",
        "  install.packages(\"textstem\")\n",
        "}\n",
        "\n",
        "# Load the textstem package\n",
        "library(textstem)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8hecXMCQDM_",
        "outputId": "43d94401-168c-42a9-b888-b579ff619a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading required package: textstem\n",
            "\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘textstem’”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘zoo’, ‘dtt’, ‘ISOcodes’, ‘sylly.en’, ‘sylly’, ‘syuzhet’, ‘fastmatch’, ‘RcppParallel’, ‘stopwords’, ‘RcppArmadillo’, ‘english’, ‘mgsub’, ‘qdapRegex’, ‘koRpus.lang.en’, ‘hunspell’, ‘koRpus’, ‘lexicon’, ‘quanteda’, ‘textclean’, ‘textshape’\n",
            "\n",
            "\n",
            "Loading required package: koRpus.lang.en\n",
            "\n",
            "Loading required package: koRpus\n",
            "\n",
            "Loading required package: sylly\n",
            "\n",
            "For information on available language packages for 'koRpus', run\n",
            "\n",
            "  available.koRpus.lang()\n",
            "\n",
            "and see ?install.koRpus.lang()\n",
            "\n",
            "\n",
            "\n",
            "Attaching package: ‘koRpus’\n",
            "\n",
            "\n",
            "The following object is masked from ‘package:tm’:\n",
            "\n",
            "    readTagged\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatization <- function(text) {\n",
        "  corpus <- Corpus(VectorSource(text))\n",
        "  corpus <- tm_map(corpus, lemmatize_strings)\n",
        "  return(corpus)\n",
        "}\n",
        "\n",
        "lemmatized_corpus <- lemmatization(text)\n",
        "\n",
        "print(\"Lemmatized Tokens:\")\n",
        "print(unlist(sapply(lemmatized_corpus, as.character)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnnH-uvIcEB3",
        "outputId": "3e9ea678-91a2-4c48-b5d3-565acb09ecb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in tm_map.SimpleCorpus(corpus, lemmatize_strings):\n",
            "“transformation drops documents”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Lemmatized Tokens:\"\n",
            "[1] \"aur bhi dukh hai zamane mai mohabbat ke siva, raheten aur bhi hai vasl ke rahat ke siva\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scrape data from a website**"
      ],
      "metadata": {
        "id": "PioVGFwccHvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and load required libraries\n",
        "install.packages(\"rvest\")\n",
        "library(rvest)\n",
        "\n",
        "# Function to scrape text within <p> tags from a website\n",
        "scrape_website <- function(url) {\n",
        "  webpage <- read_html(url)\n",
        "  paragraphs <- html_nodes(webpage, \"p\")  # Select only <p> tags\n",
        "  text <- html_text(paragraphs)\n",
        "  return(text)\n",
        "}\n",
        "\n",
        "# URL of the website to scrape\n",
        "url <- \"https://toscrape.com/\"\n",
        "\n",
        "# Scrape data from the website\n",
        "paragraphs_text <- scrape_website(url)\n",
        "\n",
        "# Print the scraped text within <p> tags\n",
        "cat(paragraphs_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGr1d2P3cJTL",
        "outputId": "77b6665b-e7fc-4124-da1e-a7b082fb2f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A fictional bookstore that desperately wants to be scraped. It's a safe place for beginners learning web scraping and for developers validating their scraping technologies as well. Available at: books.toscrape.com A website that lists quotes from famous people. It has many endpoints showing the quotes in many different ways, each of them including new scraping challenges for you, as described below."
          ]
        }
      ]
    }
  ]
}